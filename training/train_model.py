import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model.emotion_classifier import EmotionClassifier

# CutMixæ•°æ®å¢å¼º
class CutMix:
    """CutMixæ•°æ®å¢å¼ºå®ç°"""
    def __init__(self, alpha=1.0, prob=0.5):
        self.alpha = alpha
        self.prob = prob
    
    def __call__(self, batch):
        images, labels = batch
        
        if np.random.rand() > self.prob:
            return images, labels  # è¿”å›åŸå§‹æ•°æ®ï¼Œä¸åº”ç”¨CutMix
            
        batch_size = images.size(0)
        
        # ç”Ÿæˆlambdaå‚æ•°
        lam = np.random.beta(self.alpha, self.alpha)
        
        # éšæœºé€‰æ‹©ä¸¤ä¸ªæ ·æœ¬è¿›è¡Œæ··åˆ
        rand_index = torch.randperm(batch_size).to(images.device)
        
        # ç”Ÿæˆéšæœºè£å‰ªæ¡†
        W = images.size(3)
        H = images.size(2)
        cut_rat = np.sqrt(1. - lam)
        cut_w = int(W * cut_rat)
        cut_h = int(H * cut_rat)
        
        # éšæœºé€‰æ‹©è£å‰ªä½ç½®
        cx = np.random.randint(W)
        cy = np.random.randint(H)
        
        bbx1 = np.clip(cx - cut_w // 2, 0, W)
        bby1 = np.clip(cy - cut_h // 2, 0, H)
        bbx2 = np.clip(cx + cut_w // 2, 0, W)
        bby2 = np.clip(cy + cut_h // 2, 0, H)
        
        # æ‰§è¡ŒCutMix
        images[:, :, bby1:bby2, bbx1:bbx2] = images[rand_index, :, bby1:bby2, bbx1:bbx2]
        
        # è°ƒæ•´lambda
        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
        
        return images, labels, rand_index, lam

class CutMixCriterion:
    """CutMixæŸå¤±å‡½æ•°"""
    def __init__(self, criterion):
        self.criterion = criterion
    
    def __call__(self, outputs, labels_a, labels_b, lam):
        return lam * self.criterion(outputs, labels_a) + (1 - lam) * self.criterion(outputs, labels_b)

class EmotionTrainer:
    """æƒ…ç»ªè¯†åˆ«æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, data_dir, model_save_path, batch_size=32, learning_rate=0.001, num_epochs=20, 
                 resume_from_checkpoint=True, use_cutmix=True, cutmix_alpha=1.0, cutmix_prob=0.5):
        self.data_dir = data_dir
        self.model_save_path = model_save_path
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs
        self.resume_from_checkpoint = resume_from_checkpoint
        self.use_cutmix = use_cutmix
        
        # æ£€æŸ¥è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = EmotionClassifier(num_classes=7)
        self.model.to(self.device)
        
        # å‡†å¤‡æ•°æ®
        self.prepare_data()
        
        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = nn.CrossEntropyLoss()
        self.cutmix_criterion = CutMixCriterion(self.criterion)
        
        # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œæ›´å¥½çš„æƒé‡è¡°å‡
        self.optimizer = optim.AdamW(self.model.parameters(), lr=self.learning_rate, 
                                     weight_decay=0.01, eps=1e-8)
        
        # ä½¿ç”¨Cosine Annealingå­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=5, T_mult=2, eta_min=1e-6
        )
        
        # CutMixå¢å¼º
        if self.use_cutmix:
            self.cutmix = CutMix(alpha=cutmix_alpha, prob=cutmix_prob)
            print(f"âœ… å¯ç”¨CutMixæ•°æ®å¢å¼º (alpha={cutmix_alpha}, prob={cutmix_prob})")
        
        # è®­ç»ƒå†å²è®°å½•
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        self.start_epoch = 0
        
        # å°è¯•ä»checkpointæ¢å¤è®­ç»ƒ
        self.load_checkpoint()
        
    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
        data_transforms = self.model.get_data_transforms()
        
        # åŠ è½½æ•°æ®é›†
        train_dataset = datasets.ImageFolder(
            os.path.join(self.data_dir, 'train'),
            transform=data_transforms['train']
        )
        
        val_dataset = datasets.ImageFolder(
            os.path.join(self.data_dir, 'test'),
            transform=data_transforms['val']
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_dataset, 
            batch_size=self.batch_size, 
            shuffle=True, 
            num_workers=0  # åœ¨Macä¸Šä½¿ç”¨0é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        self.val_loader = DataLoader(
            val_dataset, 
            batch_size=self.batch_size, 
            shuffle=False, 
            num_workers=0  # åœ¨Macä¸Šä½¿ç”¨0é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        # æ‰“å°æ•°æ®é›†ä¿¡æ¯
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        print(f"ç±»åˆ«æ•°: {len(train_dataset.classes)}")
        print(f"ç±»åˆ«: {train_dataset.classes}")
        
        # ä¿å­˜ç±»åˆ«æ ‡ç­¾
        self.class_names = train_dataset.classes
        
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch - æ”¯æŒCutMix"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc='è®­ç»ƒä¸­')
        for batch_idx, (inputs, labels) in enumerate(pbar):
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            
            # åº”ç”¨CutMixå¢å¼º
            if self.use_cutmix:
                cutmix_result = self.cutmix((inputs, labels))
                if len(cutmix_result) == 4:  # CutMixè¢«åº”ç”¨
                    inputs, labels_a, rand_index, lam = cutmix_result
                    labels_b = labels[rand_index]
                    
                    # å‰å‘ä¼ æ’­
                    self.optimizer.zero_grad()
                    outputs = self.model(inputs)
                    loss = self.cutmix_criterion(outputs, labels_a, labels_b, lam)
                    
                    # è®¡ç®—å‡†ç¡®ç‡ (CutMixä¸‹çš„è¿‘ä¼¼è®¡ç®—)
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (lam * predicted.eq(labels_a).cpu().sum().float() + 
                              (1 - lam) * predicted.eq(labels_b).cpu().sum().float())
                else:
                    # CutMixæœªè¢«åº”ç”¨ï¼Œä½¿ç”¨æ­£å¸¸è®­ç»ƒ
                    inputs, labels = cutmix_result
                    self.optimizer.zero_grad()
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, labels)
                    
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
            else:
                # æ­£å¸¸è®­ç»ƒï¼ˆä¸ä½¿ç”¨CutMixï¼‰
                self.optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.6f}'
            })
        
        epoch_loss = running_loss / len(self.train_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc
    
    def validate_epoch(self):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc='éªŒè¯ä¸­')
            for inputs, labels in pbar:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100.*correct/total:.2f}%'
                })
        
        epoch_loss = running_loss / len(self.val_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc
    
    def train(self):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        print("å¼€å§‹è®­ç»ƒ...")
        best_val_acc = max(self.val_accuracies) if self.val_accuracies else 0.0
        
        if self.start_epoch > 0:
            print(f"ä»epoch {self.start_epoch} ç»§ç»­è®­ç»ƒï¼Œå½“å‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        
        for epoch in range(self.start_epoch, self.num_epochs):
            print(f'\nEpoch {epoch+1}/{self.num_epochs}')
            print('-' * 50)
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch()
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_acc)
            
            # éªŒè¯
            val_loss, val_acc = self.validate_epoch()
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            print(f'è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%')
            print(f'éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%')
            print(f'å­¦ä¹ ç‡: {self.optimizer.param_groups[0]["lr"]:.6f}')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                self.save_model(epoch, val_acc, is_best=True)
                print(f'ä¿å­˜æœ€ä½³æ¨¡å‹! éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%')
        
        print(f'\nè®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%')
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()
        
    def save_model(self, epoch, accuracy, is_best=False):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(os.path.dirname(self.model_save_path), exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'accuracy': accuracy,
            'class_names': self.class_names,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
        }
        
        if is_best:
            torch.save(checkpoint, self.model_save_path)
            print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_save_path}")
        
        # ä¹Ÿä¿å­˜æœ€æ–°çš„æ¨¡å‹
        latest_path = self.model_save_path.replace('.pt', '_latest.pt')
        torch.save(checkpoint, latest_path)
    
    def load_checkpoint(self):
        """ä»checkpointæ¢å¤è®­ç»ƒ"""
        if not self.resume_from_checkpoint:
            print("è·³è¿‡checkpointæ¢å¤ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
            return
            
        # ä¼˜å…ˆå°è¯•åŠ è½½æœ€æ–°çš„æ¨¡å‹
        latest_path = self.model_save_path.replace('.pt', '_latest.pt')
        checkpoint_path = None
        
        if os.path.exists(latest_path):
            checkpoint_path = latest_path
            print(f"å‘ç°æœ€æ–°checkpoint: {latest_path}")
        elif os.path.exists(self.model_save_path):
            checkpoint_path = self.model_save_path
            print(f"å‘ç°æœ€ä½³checkpoint: {self.model_save_path}")
        else:
            print("æœªæ‰¾åˆ°checkpointæ–‡ä»¶ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
            return
            
        try:
            print(f"æ­£åœ¨åŠ è½½checkpoint: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # åŠ è½½æ¨¡å‹æƒé‡
            self.model.load_state_dict(checkpoint['model_state_dict'])
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
            try:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            except Exception as e:
                print(f"âš ï¸  ä¼˜åŒ–å™¨çŠ¶æ€ä¸å…¼å®¹ï¼ˆä»Adamâ†’AdamWï¼‰: {e}")
                print("ä½¿ç”¨æ–°çš„AdamWä¼˜åŒ–å™¨çŠ¶æ€")
            
            # åŠ è½½è®­ç»ƒå†å²
            if 'train_losses' in checkpoint:
                self.train_losses = checkpoint['train_losses']
                self.val_losses = checkpoint['val_losses']
                self.train_accuracies = checkpoint['train_accuracies']
                self.val_accuracies = checkpoint['val_accuracies']
                
            # è®¾ç½®èµ·å§‹epoch
            self.start_epoch = checkpoint.get('epoch', 0) + 1
            
            print(f"âœ… æˆåŠŸä»checkpointæ¢å¤è®­ç»ƒ:")
            print(f"   - å·²å®Œæˆepoch: {checkpoint.get('epoch', 0)}")
            print(f"   - ä¸Šæ¬¡éªŒè¯å‡†ç¡®ç‡: {checkpoint.get('accuracy', 0):.2f}%")
            print(f"   - å°†ä»epoch {self.start_epoch} ç»§ç»­è®­ç»ƒ")
            
        except Exception as e:
            print(f"âŒ åŠ è½½checkpointå¤±è´¥: {e}")
            print("ä»å¤´å¼€å§‹è®­ç»ƒ")
            self.start_epoch = 0
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'class_names': self.class_names,
            'training_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'num_epochs': self.num_epochs,
            'batch_size': self.batch_size,
            'learning_rate': self.learning_rate
        }
        
        history_path = self.model_save_path.replace('.pt', '_history.json')
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        
        print(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {history_path}")
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
import matplotlib.pyplot as plt

class ModelTrainer:
    def __init__(self):
        # Dummy data for demonstration purposes
        self.train_losses = [0.5, 0.4, 0.3, 0.2, 0.1]
        self.val_losses = [0.6, 0.5, 0.4, 0.3, 0.2]
        self.train_accuracies = [80, 82, 85, 88, 90]
        self.val_accuracies = [78, 81, 84, 86, 88]
        self.model_save_path = 'model.pt'

    def plot_training_curves(self):
        """Plot training curves"""
        plt.figure(figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        plt.subplot(1, 2, 1)
        plt.plot(self.train_losses, label='Training Loss', color='blue')
        plt.plot(self.val_losses, label='Validation Loss', color='orange')
        plt.title('Training Loss Curve')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(1, 2, 2)
        plt.plot(self.train_accuracies, label='Training Accuracy', color='blue')
        plt.plot(self.val_accuracies, label='Validation Accuracy', color='orange')
        plt.title('Training Accuracy Curve')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = self.model_save_path.replace('.pt', '_training_curves.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜åˆ°: {plot_path}")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è·¯å¾„
    data_dir = 'dataset'  # æ•°æ®é›†ç›®å½•
    model_save_path = 'model/emotion_model.pt'  # æ¨¡å‹ä¿å­˜è·¯å¾„
    
    print("ğŸš€ å¯åŠ¨å¢å¼ºå‹æƒ…ç»ªè¯†åˆ«æ¨¡å‹è®­ç»ƒ")
    print("ğŸ“Š æ¨¡å‹æ¶æ„: ResNet18 (å¢å¼ºç‰ˆ)")
    print("ğŸ¯ æ•°æ®å¢å¼º: CutMix + é«˜çº§å¢å¼ºç­–ç•¥")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = EmotionTrainer(
        data_dir=data_dir,
        model_save_path=model_save_path,
        batch_size=16,              # å‡å°batch sizeæé«˜ç¨³å®šæ€§
        learning_rate=0.0005,       # é™ä½å­¦ä¹ ç‡
        num_epochs=15,              # å¢åŠ è®­ç»ƒè½®æ•°
        resume_from_checkpoint=True,  # å¯ç”¨æ–­ç‚¹ç»­è®­
        use_cutmix=True,            # å¯ç”¨CutMix
        cutmix_alpha=1.0,           # CutMixå‚æ•°
        cutmix_prob=0.5             # CutMixæ¦‚ç‡
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()

if __name__ == '__main__':
    main() 