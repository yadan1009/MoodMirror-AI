import os
import torch
import cv2
import numpy as np
from PIL import Image
import torch.nn.functional as F
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model.emotion_classifier import EmotionClassifier
from app.storage import emotion_storage

class EmotionPredictor:
    """æƒ…ç»ªé¢„æµ‹å™¨"""
    
    def __init__(self, model_path='model/emotion_model.pt', device=None):
        self.model_path = model_path
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.face_cascade = None
        self.is_loaded = False
        
        # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
        self.init_face_detector()
        
        # å°è¯•åŠ è½½æ¨¡å‹
        if os.path.exists(model_path):
            self.load_model()
        else:
            print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹")
    
    def init_face_detector(self):
        """åˆå§‹åŒ–OpenCVäººè„¸æ£€æµ‹å™¨"""
        try:
            # å°è¯•åŠ è½½Haarçº§è”åˆ†ç±»å™¨
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            
            if self.face_cascade.empty():
                print("è­¦å‘Š: æ— æ³•åŠ è½½äººè„¸æ£€æµ‹å™¨")
                self.face_cascade = None
            else:
                print("äººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
                
        except Exception as e:
            print(f"åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨å¤±è´¥: {e}")
            self.face_cascade = None
    
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
            
            # åŠ è½½checkpoint
            checkpoint = torch.load(self.model_path, map_location=self.device)
            
            # åˆå§‹åŒ–æ¨¡å‹ï¼ˆåŠ è½½å·²è®­ç»ƒæ¨¡å‹æ—¶ä¸éœ€è¦é¢„è®­ç»ƒæƒé‡ï¼‰
            self.model = EmotionClassifier(num_classes=7, load_pretrained=False)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            accuracy = checkpoint.get('accuracy', 0)
            epoch = checkpoint.get('epoch', 0)
            
            print(f"æ¨¡å‹åŠ è½½æˆåŠŸ!")
            print(f"è®­ç»ƒè½®æ•°: {epoch}, éªŒè¯å‡†ç¡®ç‡: {accuracy:.2f}%")
            
            self.is_loaded = True
            
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            self.model = None
            self.is_loaded = False
    
    def detect_faces(self, image):
        """
        æ£€æµ‹å›¾åƒä¸­çš„äººè„¸
        
        Args:
            image: PIL Imageå¯¹è±¡æˆ–numpyæ•°ç»„
            
        Returns:
            list: æ£€æµ‹åˆ°çš„äººè„¸ä½ç½®åˆ—è¡¨ [(x, y, w, h), ...]
        """
        if self.face_cascade is None:
            return []
        
        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        if isinstance(image, Image.Image):
            opencv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        else:
            opencv_image = image
        
        # è½¬ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
        
        # æ£€æµ‹äººè„¸
        faces = self.face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(30, 30)
        )
        
        # ç¡®ä¿è¿”å›listæ ¼å¼
        if isinstance(faces, tuple):
            return list(faces) if faces else []
        elif hasattr(faces, 'tolist'):
            return faces.tolist()
        else:
            return list(faces) if len(faces) > 0 else []
    
    def crop_face(self, image, face_coords):
        """
        ä»å›¾åƒä¸­è£å‰ªäººè„¸åŒºåŸŸ
        
        Args:
            image: PIL Imageå¯¹è±¡
            face_coords: äººè„¸åæ ‡ (x, y, w, h)
            
        Returns:
            PIL.Image: è£å‰ªåçš„äººè„¸å›¾åƒ
        """
        x, y, w, h = face_coords
        
        # æ·»åŠ ä¸€äº›è¾¹è·
        margin = 0.2
        mx = int(w * margin)
        my = int(h * margin)
        
        # è®¡ç®—è£å‰ªåŒºåŸŸ
        x1 = max(0, x - mx)
        y1 = max(0, y - my)
        x2 = min(image.width, x + w + mx)
        y2 = min(image.height, y + h + my)
        
        # è£å‰ªå›¾åƒ
        face_image = image.crop((x1, y1, x2, y2))
        
        return face_image
    
    def predict_emotion_from_image(self, image, save_to_db=True):
        """
        ä»å›¾åƒé¢„æµ‹æƒ…ç»ª
        
        Args:
            image: PIL Imageå¯¹è±¡æˆ–å›¾åƒè·¯å¾„
            save_to_db: æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
            
        Returns:
            dict: é¢„æµ‹ç»“æœ
        """
        if not self.is_loaded:
            # æ¼”ç¤ºæ¨¡å¼ï¼šä½¿ç”¨éšæœºé¢„æµ‹
            print("ğŸ­ æ¼”ç¤ºæ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹Ÿæƒ…ç»ªè¯†åˆ«")
            return self._demo_prediction(image, save_to_db)
        
        try:
            # å¦‚æœæ˜¯è·¯å¾„ï¼Œå…ˆåŠ è½½å›¾åƒ
            if isinstance(image, str):
                if not os.path.exists(image):
                    return {'error': f'å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image}'}
                image = Image.open(image).convert('RGB')
            
            # æ£€æµ‹äººè„¸
            faces = self.detect_faces(image)
            
            if len(faces) == 0:
                # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œç›´æ¥ä½¿ç”¨æ•´å¼ å›¾åƒ
                print("æœªæ£€æµ‹åˆ°äººè„¸ï¼Œä½¿ç”¨æ•´å¼ å›¾åƒè¿›è¡Œé¢„æµ‹")
                face_image = image
            else:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
                print(f"æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªäººè„¸")
                face_image = self.crop_face(image, faces[0])
            
            # é¢„å¤„ç†å›¾åƒå¹¶ä½¿ç”¨æ¨¡å‹é¢„æµ‹
            image_tensor = self.model.preprocess_image(face_image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(image_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                predicted_idx = predicted.item()
                confidence_score = confidence.item()
                
                result = {
                    'emotion': self.model.emotion_labels[predicted_idx],
                    'emotion_cn': self.model.emotion_labels_cn[predicted_idx],
                    'confidence': confidence_score,
                    'all_probabilities': probabilities.cpu().numpy().flatten()
                }
            
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            result['faces_detected'] = len(faces)
            result['model_loaded'] = True
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if save_to_db:
                emotion_storage.add_emotion_record(
                    emotion=result['emotion'],
                    emotion_cn=result['emotion_cn'],
                    confidence=result['confidence'],
                    all_probabilities=result['all_probabilities']
                )
            
            return result
            
        except Exception as e:
            print(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {
                'error': str(e),
                'emotion': 'error',
                'emotion_cn': 'é”™è¯¯',
                'confidence': 0.0
            }
    
    def predict_emotion_from_webcam(self, save_to_db=True):
        """
        ä»æ‘„åƒå¤´æ•è·å›¾åƒå¹¶é¢„æµ‹æƒ…ç»ª
        
        Args:
            save_to_db: æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
            
        Returns:
            dict: é¢„æµ‹ç»“æœ
        """
        try:
            # æ‰“å¼€æ‘„åƒå¤´
            cap = cv2.VideoCapture(0)
            
            if not cap.isOpened():
                return {'error': 'æ— æ³•æ‰“å¼€æ‘„åƒå¤´'}
            
            print("æŒ‰ç©ºæ ¼é”®æ‹ç…§ï¼ŒæŒ‰ESCé”®é€€å‡º")
            
            while True:
                # è¯»å–å¸§
                ret, frame = cap.read()
                if not ret:
                    return {'error': 'æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢'}
                
                # æ˜¾ç¤ºç”»é¢
                cv2.imshow('æƒ…ç»ªè¯†åˆ« - æŒ‰ç©ºæ ¼é”®æ‹ç…§', frame)
                
                key = cv2.waitKey(1) & 0xFF
                
                # ç©ºæ ¼é”®æ‹ç…§
                if key == ord(' '):
                    # è½¬æ¢ä¸ºPILå›¾åƒ
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(rgb_frame)
                    
                    # é¢„æµ‹æƒ…ç»ª
                    result = self.predict_emotion_from_image(pil_image, save_to_db)
                    
                    # æ˜¾ç¤ºç»“æœ
                    print(f"æ£€æµ‹ç»“æœ: {result['emotion_cn']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
                    
                    cap.release()
                    cv2.destroyAllWindows()
                    
                    return result
                
                # ESCé”®é€€å‡º
                elif key == 27:
                    cap.release()
                    cv2.destroyAllWindows()
                    return {'error': 'ç”¨æˆ·å–æ¶ˆæ“ä½œ'}
            
        except Exception as e:
            return {'error': f'æ‘„åƒå¤´æ“ä½œå¤±è´¥: {e}'}
    
    def batch_predict(self, image_paths, save_to_db=False):
        """
        æ‰¹é‡é¢„æµ‹å›¾åƒæƒ…ç»ª
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            save_to_db: æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
            
        Returns:
            list: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        
        for image_path in image_paths:
            print(f"å¤„ç†å›¾åƒ: {image_path}")
            result = self.predict_emotion_from_image(image_path, save_to_db)
            result['image_path'] = image_path
            results.append(result)
        
        return results
    
    def get_emotion_emoji(self, emotion):
        """
        æ ¹æ®æƒ…ç»ªè·å–å¯¹åº”çš„emoji
        
        Args:
            emotion: è‹±æ–‡æƒ…ç»ªæ ‡ç­¾
            
        Returns:
            str: emojiå­—ç¬¦
        """
        emoji_map = {
            'happy': 'ğŸ˜„',
            'sad': 'ğŸ˜¢',
            'angry': 'ğŸ˜ ',
            'fear': 'ğŸ˜¨',
            'surprise': 'ğŸ˜²',
            'disgust': 'ğŸ¤¢',
            'neutral': 'ğŸ˜',
            'unknown': 'â“',
            'error': 'âŒ'
        }
        
        return emoji_map.get(emotion, 'â“')
    
    def _demo_prediction(self, image, save_to_db=True):
        """
        æ¼”ç¤ºæ¨¡å¼çš„æƒ…ç»ªé¢„æµ‹ï¼ˆä½¿ç”¨éšæœºé¢„æµ‹ï¼‰
        
        Args:
            image: PIL Imageå¯¹è±¡æˆ–å›¾åƒè·¯å¾„
            save_to_db: æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
            
        Returns:
            dict: æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
        """
        import random
        import numpy as np
        
        try:
            # å¦‚æœæ˜¯è·¯å¾„ï¼Œå…ˆåŠ è½½å›¾åƒ
            if isinstance(image, str):
                if not os.path.exists(image):
                    return {'error': f'å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image}'}
                from PIL import Image
                image = Image.open(image).convert('RGB')
            
            # æ£€æµ‹äººè„¸
            faces = self.detect_faces(image)
            
            # æ¨¡æ‹Ÿæƒ…ç»ªé¢„æµ‹
            emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
            emotions_cn = ['æ„¤æ€’', 'åŒæ¶', 'ææƒ§', 'å¿«ä¹', 'ä¸­æ€§', 'æ‚²ä¼¤', 'æƒŠè®¶']
            
            # ç”Ÿæˆåˆç†çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆä¸»è¦æƒ…ç»ªæ¦‚ç‡è¾ƒé«˜ï¼‰
            predicted_idx = random.randint(0, 6)
            probabilities = np.random.dirichlet([0.5] * 7)  # ç”Ÿæˆå’Œä¸º1çš„æ¦‚ç‡åˆ†å¸ƒ
            
            # å¢å¼ºä¸»è¦æƒ…ç»ªçš„æ¦‚ç‡
            probabilities[predicted_idx] = max(probabilities[predicted_idx], 0.4 + random.random() * 0.4)
            probabilities = probabilities / probabilities.sum()  # é‡æ–°å½’ä¸€åŒ–
            
            confidence = probabilities[predicted_idx]
            
            result = {
                'emotion': emotions[predicted_idx],
                'emotion_cn': emotions_cn[predicted_idx],
                'confidence': float(confidence),
                'all_probabilities': probabilities,
                'faces_detected': len(faces),
                'model_loaded': False,
                'demo_mode': True
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if save_to_db:
                emotion_storage.add_emotion_record(
                    emotion=result['emotion'],
                    emotion_cn=result['emotion_cn'],
                    confidence=result['confidence'],
                    all_probabilities=result['all_probabilities']
                )
                print(f"ğŸ­ æ¼”ç¤ºæ¨¡å¼é¢„æµ‹: {result['emotion_cn']} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            return result
            
        except Exception as e:
            print(f"æ¼”ç¤ºé¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {
                'error': str(e),
                'emotion': 'error',
                'emotion_cn': 'é”™è¯¯',
                'confidence': 0.0,
                'demo_mode': True
            }

# åˆ›å»ºå…¨å±€é¢„æµ‹å™¨å®ä¾‹
emotion_predictor = EmotionPredictor()

def test_predictor():
    """æµ‹è¯•é¢„æµ‹å™¨åŠŸèƒ½"""
    predictor = EmotionPredictor()
    
    if predictor.is_loaded:
        print("æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¯ä»¥å¼€å§‹é¢„æµ‹")
        
        # æµ‹è¯•æ‘„åƒå¤´é¢„æµ‹ï¼ˆå¦‚æœæœ‰æ‘„åƒå¤´çš„è¯ï¼‰
        # result = predictor.predict_emotion_from_webcam()
        # print(f"é¢„æµ‹ç»“æœ: {result}")
    else:
        print("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")

if __name__ == '__main__':
    test_predictor() 