import torch
import torch.nn as nn
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import numpy as np
import os

class EmotionClassifier(nn.Module):
    """
    åŸºäºResNet18çš„æƒ…ç»ªè¯†åˆ«æ¨¡å‹
    æ”¯æŒ7ç§æƒ…ç»ªåˆ†ç±»ï¼šangry, disgust, fear, happy, neutral, sad, surprise
    """
    
    def __init__(self, num_classes=7, load_pretrained=True):
        super(EmotionClassifier, self).__init__()
        self.num_classes = num_classes
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        if load_pretrained:
            try:
                self.backbone = models.resnet18(pretrained=True)
                print("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒçš„ResNet18æ¨¡å‹")
            except Exception as e:
                print(f"âš ï¸  åœ¨çº¿ä¸‹è½½å¤±è´¥: {e}")
                # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
                try:
                    import torch
                    cache_path = torch.hub.get_dir()
                    model_path = f"{cache_path}/checkpoints/resnet18-f37072fd.pth"
                    if os.path.exists(model_path):
                        print("ğŸ”„ å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½é¢„è®­ç»ƒæƒé‡...")
                        self.backbone = models.resnet18(pretrained=False)
                        state_dict = torch.load(model_path, map_location='cpu')
                        self.backbone.load_state_dict(state_dict)
                        print("âœ… æˆåŠŸä»æœ¬åœ°ç¼“å­˜åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼")
                    else:
                        print("ğŸ“ æœ¬åœ°ç¼“å­˜ä¸å­˜åœ¨ï¼Œä½¿ç”¨æœªé¢„è®­ç»ƒæ¨¡å‹")
                        self.backbone = models.resnet18(pretrained=False)
                except Exception as e2:
                    print(f"âš ï¸  æœ¬åœ°åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                    self.backbone = models.resnet18(pretrained=False)
        else:
            self.backbone = models.resnet18(pretrained=False)
            print("ğŸ”§ åŠ è½½å·²è®­ç»ƒæ¨¡å‹ï¼Œè·³è¿‡é¢„è®­ç»ƒæƒé‡ä¸‹è½½")
        
        # æ›¿æ¢æœ€åçš„å…¨è¿æ¥å±‚
        self.backbone.fc = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(self.backbone.fc.in_features, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
        # æƒ…ç»ªæ ‡ç­¾æ˜ å°„
        self.emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
        self.emotion_labels_cn = ['æ„¤æ€’', 'åŒæ¶', 'ææƒ§', 'å¿«ä¹', 'ä¸­æ€§', 'æ‚²ä¼¤', 'æƒŠè®¶']
        
    def forward(self, x):
        return self.backbone(x)
    
    def predict_emotion(self, image_path_or_tensor, device='cpu'):
        """
        é¢„æµ‹å•å¼ å›¾ç‰‡çš„æƒ…ç»ª
        
        Args:
            image_path_or_tensor: å›¾ç‰‡è·¯å¾„æˆ–å·²å¤„ç†çš„tensor
            device: è®¾å¤‡ç±»å‹
            
        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        self.eval()
        with torch.no_grad():
            if isinstance(image_path_or_tensor, str):
                # å¦‚æœæ˜¯è·¯å¾„ï¼Œå…ˆåŠ è½½å›¾ç‰‡
                image = Image.open(image_path_or_tensor).convert('RGB')
                image_tensor = self.preprocess_image(image).unsqueeze(0).to(device)
            elif hasattr(image_path_or_tensor, 'save'):
                # å¦‚æœæ˜¯PIL Imageå¯¹è±¡
                image_tensor = self.preprocess_image(image_path_or_tensor).unsqueeze(0).to(device)
            else:
                # å¦‚æœå·²ç»æ˜¯tensor
                image_tensor = image_path_or_tensor.to(device)
            
            # æ¨¡å‹æ¨ç†
            outputs = self.forward(image_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            
            # è·å–é¢„æµ‹ç»“æœ
            confidence, predicted = torch.max(probabilities, 1)
            predicted_idx = predicted.item()
            confidence_score = confidence.item()
            
            return {
                'emotion': self.emotion_labels[predicted_idx],
                'emotion_cn': self.emotion_labels_cn[predicted_idx],
                'confidence': confidence_score,
                'all_probabilities': probabilities.cpu().numpy().flatten()
            }
    
    @staticmethod
    def preprocess_image(image):
        """
        å›¾ç‰‡é¢„å¤„ç†
        
        Args:
            image: PIL Imageå¯¹è±¡
            
        Returns:
            torch.Tensor: é¢„å¤„ç†åçš„tensor
        """
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        return transform(image)
    
    def get_data_transforms(self):
        """
        è·å–è®­ç»ƒå’ŒéªŒè¯ç”¨çš„æ•°æ®å˜æ¢
        
        Returns:
            dict: åŒ…å«trainå’Œvalçš„transforms
        """
        data_transforms = {
            'train': transforms.Compose([
                transforms.Resize((256, 256)),
                transforms.RandomCrop(224),
                transforms.RandomHorizontalFlip(p=0.6),
                transforms.RandomRotation(degrees=20),
                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1),
                transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15)),
                # æ·»åŠ æ›´å¤šå¢å¼ºç­–ç•¥
                transforms.RandomPerspective(distortion_scale=0.2, p=0.3),
                transforms.ToTensor(),
                # åœ¨ToTensorä¹‹ååº”ç”¨çš„å¢å¼º
                transforms.RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3)),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ]),
            'val': transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
        }
        return data_transforms 